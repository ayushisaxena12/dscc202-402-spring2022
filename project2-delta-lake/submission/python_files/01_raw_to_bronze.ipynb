{"cells":[{"cell_type":"markdown","source":["# Raw to Bronze Pattern"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"222e10fc-2a31-4b6d-8553-f0d3670ba8e0"}}},{"cell_type":"markdown","source":["## Notebook Objective\n\nIn this notebook we:\n1. Ingest Raw Data\n2. Augment the data with Ingestion Metadata\n3. Stream write the augmented data to a Bronze Table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7951ffb8-0070-4536-9ec3-68fa9dda59f3"}}},{"cell_type":"markdown","source":["## Step Configuration"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8512d3e9-f6d5-4898-a036-0e0436f6c387"}}},{"cell_type":"code","source":["%run ./includes/configuration"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce620d56-b153-4387-8442-9c657792589b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[37]: DataFrame[]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[37]: DataFrame[]"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"No running streams.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["No running streams.\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Display the Files in the Raw Path"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60731864-45a8-4868-a6ad-3b6cccab8095"}}},{"cell_type":"code","source":["display(dbutils.fs.ls(rawPath))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ab6f13b-f4e3-4595-a745-a8d38f7bd855"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/dbacademy/ayushi_saxena/dataengineering/plus/raw/health_tracker_data_2020_1.json","health_tracker_data_2020_1.json",310628]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/dbacademy/ayushi_saxena/dataengineering/plus/raw/health_tracker_data_2020_1.json</td><td>health_tracker_data_2020_1.json</td><td>310628</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Make Notebook Idempotent"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db56f2e0-a6c4-4d0a-8034-551d480917b0"}}},{"cell_type":"code","source":["dbutils.fs.rm(bronzePath, recurse=True)\ndbutils.fs.rm(bronzeCheckpoint, recurse=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7b4dacae-1ec7-45f9-bf1c-6ff548d4c185"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[41]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[41]: True"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Ingest raw data\n\nNext, we will stream files from the source directory and write each line as a string to the Bronze table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e55066bc-dc4a-4b42-8726-c9a28c6071a5"}}},{"cell_type":"code","source":["kafka_schema = \"value STRING\"\n\nraw_health_tracker_data_df = (\n    spark.readStream.format(\"text\").schema(kafka_schema).load(rawPath)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ac6a5e1-3dab-4be6-8763-b8c43aea1d49"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Exercise:** Write an Assertion Statement to Verify the Schema of the Raw Data\n\nAt this point, we write an assertion statement to verify that our streaming DataFrame has the schema we expect.\n\nYour assertion should make sure that the `raw_health_tracker_data_df` DataFrame has the correct schema.\n\nü§† The function `_parse_datatype_string` (read more [here](http://spark.apache.org/docs/2.1.2/api/python/_modules/pyspark/sql/types.html)) converts a DDL format schema string into a Spark schema."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1bfbaecb-2cd9-4703-a50d-262c1b8eb5b5"}}},{"cell_type":"code","source":["# TODO\nfrom pyspark.sql.types import _parse_datatype_string\nassert raw_health_tracker_data_df.schema == _parse_datatype_string(kafka_schema), \"File not present in Raw Path\"\nprint(\"Assertion passed.\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1a96fbb-f389-4861-81b4-5c112adf6074"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Assertion passed.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Assertion passed.\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Display the Raw Data\n\nü§ì Each row here is a raw string in JSON format, as would be passed by a stream server like Kafka."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"337cff33-a05b-4514-85de-60efad3f86c8"}}},{"cell_type":"code","source":["display(raw_health_tracker_data_df, streamName=\"display_raw\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac91a031-2e46-4f9f-a2e5-0d03a0d55260"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["‚ùóÔ∏è To prevent the `display` function from continuously streaming, run the following utility function."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c25df342-3371-4e73-83a5-a098155a380d"}}},{"cell_type":"code","source":["stop_named_stream(spark, \"display_raw\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e8c5d4ff-7983-450e-8d86-13e8385cf7ce"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[45]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[45]: True"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Ingestion Metadata\n\nAs part of the ingestion process, we record metadata for the ingestion. In this case, we track the data sources, the ingestion time (`ingesttime`), and the ingest date (`ingestdate`) using the `pyspark.sql` functions `current_timestamp` and `lit`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"22442ad2-517b-41fe-9705-6d8bd8e8ae9e"}}},{"cell_type":"code","source":["from pyspark.sql.functions import current_timestamp, lit\n\nraw_health_tracker_data_df = raw_health_tracker_data_df.select(\n    lit(\"files.training.databricks.com\").alias(\"datasource\"),\n    current_timestamp().alias(\"ingesttime\"),\n    \"value\",\n    current_timestamp().cast(\"date\").alias(\"ingestdate\"),\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11e7cde4-7d5b-4cda-a661-fad9043ac460"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## WRITE Stream to a Bronze Table\n\nFinally, we write to the Bronze Table using Structured Streaming.\n\nüôÖüèΩ‚Äç‚ôÄÔ∏è While we _can_ write directly to tables using the `.table()` notation, this will create fully managed tables by writing output to a default location on DBFS. This is not best practice and should be avoided in nearly all cases."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ee7bd7f-ab94-40a3-8864-17a2828bb8ef"}}},{"cell_type":"markdown","source":["### Partitioning\nThis course uses a dataset that is extremely small relative to an actual production system. Still we demonstrate the best practice of partitioning by date and partition on the ingestion date, column `p_ingestdate`.\n\nüò≤ Note that we have aliased the `ingestdate` column to be `p_ingestdate`. We have done this in order to inform anyone who looks at the schema for this table that it has been partitioned by the ingestion date."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"08cbb646-d84a-4cc2-87f7-aba5ee6a438a"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\n(\n    raw_health_tracker_data_df.select(\n        \"datasource\", \"ingesttime\", \"value\", col(\"ingestdate\").alias(\"p_ingestdate\")\n    )\n    .writeStream.format(\"delta\")\n    .outputMode(\"append\")\n    .option(\"checkpointLocation\", bronzeCheckpoint)\n    .partitionBy(\"p_ingestdate\")\n    .queryName(\"write_raw_to_bronze\")\n    .start(bronzePath)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5bc3a02-7215-4d5c-a873-64dfe53034cb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[47]: <pyspark.sql.streaming.StreamingQuery at 0x7fd4903cfb20>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[47]: <pyspark.sql.streaming.StreamingQuery at 0x7fd4903cfb20>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Checkpointing\n\nWhen defining a Delta Lake streaming query, one of the options that you need to specify is the location of a checkpoint directory.\n\n`.writeStream.format(\"delta\").option(\"checkpointLocation\", <path-to-checkpoint-directory>) ...`\n\nThis is actually a structured streaming feature. It stores the current state of your streaming job.\n\nShould your streaming job stop for some reason and you restart it, it will continue from where it left off.\n\nüíÄ If you do not have a checkpoint directory, when the streaming job stops, you lose all state around your streaming job and upon restart, you start from scratch.\n\n‚úãüèΩ Also note that every streaming job should have its own checkpoint directory: no sharing."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d89807b4-d8c2-45f4-81c7-9b1d76aeac65"}}},{"cell_type":"markdown","source":["## Create a Reference to the Delta table files\n\nIn this command we create a Spark DataFrame via a reference to the Delta file in DBFS."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d4fed081-d2c9-40be-926d-2ccb315d18b5"}}},{"cell_type":"code","source":["bronze_health_tracker = spark.readStream.format(\"delta\").load(bronzePath)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e727e5d5-8a45-48de-8218-12323e8a1f1a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Troubleshooting\n\nüò´ If you try to run this before the `writeStream` above has been created, you may see the following error:\n\n`\nAnalysisException: Table schema is not set.  Write data into it or use CREATE TABLE to set the schema.;`\n\nIf this happens, wait a moment for the `writeStream` to instantiate and run the command again."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf7303eb-396d-4e4b-bd40-ba027793b0ee"}}},{"cell_type":"markdown","source":["## Display the files in the Delta table\n\nThese files can be viewed using the `dbutils.fs.ls` function."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fa558af8-1dcc-4b3b-98e4-080ffd1db7f4"}}},{"cell_type":"code","source":["display(dbutils.fs.ls(bronzePath))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a812f596-5f6f-4e91-89fa-1663da2f5b08"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/dbacademy/ayushi_saxena/dataengineering/plus/bronze/_delta_log/","_delta_log/",0],["dbfs:/dbacademy/ayushi_saxena/dataengineering/plus/bronze/p_ingestdate=2022-03-25/","p_ingestdate=2022-03-25/",0]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/dbacademy/ayushi_saxena/dataengineering/plus/bronze/_delta_log/</td><td>_delta_log/</td><td>0</td></tr><tr><td>dbfs:/dbacademy/ayushi_saxena/dataengineering/plus/bronze/p_ingestdate=2022-03-25/</td><td>p_ingestdate=2022-03-25/</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Exercise:** Write an Assertion Statement to Verify the Schema of the Bronze Delta Table\n\nAt this point, we write an assertion statement to verify that our Bronze Delta table has the schema we expect.\n\nYour assertion should make sure that the `bronze_health_tracker` DataFrame has the correct schema.\n\nüí™üèº Remember, the function `_parse_datatype_string` converts a DDL format schema string into a Spark schema."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"33664a7d-38ae-44b7-b56d-54fe27de2d4b"}}},{"cell_type":"code","source":["assert bronze_health_tracker.schema == _parse_datatype_string(\"datasource STRING, ingesttime TIMESTAMP, value STRING, p_ingestdate DATE\"), \"File not present in Bronze Path\"\nprint(\"Assertion passed.\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9701446e-0fa3-4724-afb6-1bf6910a12d2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Assertion passed.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Assertion passed.\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Display Running Streams\n\nYou can use the following code to display all streams that are currently running."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ebbab103-dff9-491f-914c-3767286a2528"}}},{"cell_type":"code","source":["for stream in spark.streams.active:\n    print(stream.name)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5d7fbab4-ccdb-4e3b-af7f-ef8d3f5f42fa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"write_raw_to_bronze\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["write_raw_to_bronze\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Register the Bronze Table in the Metastore\n\nRecall that a Delta table registered in the Metastore is a reference to a physical table created in object storage.\n\nWe just created a Bronze Delta table in object storage by writing data to a specific location. If we register that location with the Metastore as a table, we can query the tables using SQL.\n\n(Because we will never directly query the Bronze table, it is not strictly necessary to register this table in the Metastore, but we will do so for demonstration purposes.)\n\nAt Delta table creation, the Delta files in Object Storage define the schema, partitioning, and table properties. For this reason, it is not necessary to specify any of these when registering the table with the Metastore. Furthermore, no table repair is required. The transaction log stored with the Delta files contains all metadata needed for an immediate query."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"881c360e-df77-491f-b249-0898a30fda88"}}},{"cell_type":"code","source":["spark.sql(\n    \"\"\"\nDROP TABLE IF EXISTS health_tracker_plus_bronze\n\"\"\"\n)\n\nspark.sql(\n    f\"\"\"\nCREATE TABLE health_tracker_plus_bronze\nUSING DELTA\nLOCATION \"{bronzePath}\"\n\"\"\"\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2862d5e5-ae6c-42d0-8c37-009606bf4fd8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[52]: DataFrame[]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[52]: DataFrame[]"]}}],"execution_count":0},{"cell_type":"code","source":["display(\n    spark.sql(\n        \"\"\"\n  DESCRIBE DETAIL health_tracker_plus_bronze\n  \"\"\"\n    )\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"91103564-7ab7-4dcf-b946-95eb604834ee"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["delta","05db5236-7268-4da1-9af0-37b63ea05a7a","dbacademy_ayushi_saxena.health_tracker_plus_bronze",null,"dbfs:/dbacademy/ayushi_saxena/dataengineering/plus/bronze","2022-03-25T06:05:58.854+0000","2022-03-25T06:06:01.000+0000",["p_ingestdate"],1,77601,{},1,2]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"format","type":"\"string\"","metadata":"{}"},{"name":"id","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"description","type":"\"string\"","metadata":"{}"},{"name":"location","type":"\"string\"","metadata":"{}"},{"name":"createdAt","type":"\"timestamp\"","metadata":"{}"},{"name":"lastModified","type":"\"timestamp\"","metadata":"{}"},{"name":"partitionColumns","type":"{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}","metadata":"{}"},{"name":"numFiles","type":"\"long\"","metadata":"{}"},{"name":"sizeInBytes","type":"\"long\"","metadata":"{}"},{"name":"properties","type":"{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}","metadata":"{}"},{"name":"minReaderVersion","type":"\"integer\"","metadata":"{}"},{"name":"minWriterVersion","type":"\"integer\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>format</th><th>id</th><th>name</th><th>description</th><th>location</th><th>createdAt</th><th>lastModified</th><th>partitionColumns</th><th>numFiles</th><th>sizeInBytes</th><th>properties</th><th>minReaderVersion</th><th>minWriterVersion</th></tr></thead><tbody><tr><td>delta</td><td>05db5236-7268-4da1-9af0-37b63ea05a7a</td><td>dbacademy_ayushi_saxena.health_tracker_plus_bronze</td><td>null</td><td>dbfs:/dbacademy/ayushi_saxena/dataengineering/plus/bronze</td><td>2022-03-25T06:05:58.854+0000</td><td>2022-03-25T06:06:01.000+0000</td><td>List(p_ingestdate)</td><td>1</td><td>77601</td><td>Map()</td><td>1</td><td>2</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Delta Lake Python API\nDelta Lake provides programmatic APIs to examine and manipulate Delta tables.\n\nHere, we create a reference to the Bronze table using the Delta Lake Python API."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c13eb87-8383-4eb9-abfc-051cfd922046"}}},{"cell_type":"code","source":["from delta.tables import DeltaTable\n\nbronzeTable = DeltaTable.forPath(spark, bronzePath)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6ab06e7-0535-4f08-84ad-1bde2f78553c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(bronzeTable.history())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"704962d2-fb4f-4d8e-b16f-85ddc56f35f3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[0,"2022-03-25T06:06:01.000+0000","1188288607282333","ayushi@ur.rochester.edu","STREAMING UPDATE",{"outputMode":"Append","queryId":"dc346830-24f6-49c0-b689-b743e63b7624","epochId":"0"},null,["3089513805671221"],"0325-051123-9xt33lgc",null,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"3720","numOutputBytes":"77601","numAddedFiles":"1"},null]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"version","type":"\"long\"","metadata":"{}"},{"name":"timestamp","type":"\"timestamp\"","metadata":"{}"},{"name":"userId","type":"\"string\"","metadata":"{}"},{"name":"userName","type":"\"string\"","metadata":"{}"},{"name":"operation","type":"\"string\"","metadata":"{}"},{"name":"operationParameters","type":"{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}","metadata":"{}"},{"name":"job","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"jobId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"runId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobOwnerId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"triggerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"notebook","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"notebookId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"clusterId","type":"\"string\"","metadata":"{}"},{"name":"readVersion","type":"\"long\"","metadata":"{}"},{"name":"isolationLevel","type":"\"string\"","metadata":"{}"},{"name":"isBlindAppend","type":"\"boolean\"","metadata":"{}"},{"name":"operationMetrics","type":"{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}","metadata":"{}"},{"name":"userMetadata","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th></tr></thead><tbody><tr><td>0</td><td>2022-03-25T06:06:01.000+0000</td><td>1188288607282333</td><td>ayushi@ur.rochester.edu</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> dc346830-24f6-49c0-b689-b743e63b7624, epochId -> 0)</td><td>null</td><td>List(3089513805671221)</td><td>0325-051123-9xt33lgc</td><td>null</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 3720, numOutputBytes -> 77601, numAddedFiles -> 1)</td><td>null</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Stop All Streams\n\nIn the next notebook, we will stream data from the Bronze table to a Silver Delta table.\n\nBefore we do so, let's shut down all streams in this notebook."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"59286e49-03c4-4ac2-96b5-c4d05101359e"}}},{"cell_type":"code","source":["stop_all_streams()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b09bdbf4-5c8b-4a8c-b946-74c7e8779b5a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[56]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[56]: True"]}}],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"522e8ee5-3323-4831-b649-5f8d961726c0"}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b986b93c-f81e-41fe-bddc-5bffa82d68ab"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"01_raw_to_bronze","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3089513805671221}},"nbformat":4,"nbformat_minor":0}
